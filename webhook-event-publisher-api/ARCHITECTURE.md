# Webhook Event Publisher API - Architecture & Design

## ğŸ“‹ Overview

This is a **Spring Boot WebFlux** reactive microservice that acts as an **Event Gateway** for webhook events. It validates incoming webhook payloads against schemas stored in DynamoDB and publishes validated events to Apache Kafka (AWS MSK).

---

## ğŸ—ï¸ Architecture Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT APPLICATIONS                          â”‚
â”‚           (Webhook Producers sending events)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTP POST (JSON/XML/Avro)
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EVENT VALIDATION API                          â”‚
â”‚              (Spring Boot WebFlux - Reactive)                   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         CONTROLLER LAYER                                  â”‚  â”‚
â”‚  â”‚  â€¢ EventController - Publish events                       â”‚  â”‚
â”‚  â”‚  â€¢ SchemaController - Query schemas                       â”‚  â”‚
â”‚  â”‚  â€¢ CacheController - Manage cache                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         SERVICE LAYER                                     â”‚  â”‚
â”‚  â”‚  â€¢ CachingSchemaService (Primary, with Caffeine cache)    â”‚  â”‚
â”‚  â”‚  â€¢ DynamoSchemaService (Delegate, queries DynamoDB)       â”‚  â”‚
â”‚  â”‚  â€¢ AvroSchemaValidator - Validates payloads               â”‚  â”‚
â”‚  â”‚  â€¢ FormatConverter - XML/JSON/Avro conversion             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                    â”‚                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   PUBLISHER LAYER        â”‚  â”‚   CONFIGURATION             â”‚â”‚
â”‚  â”‚ â€¢ KafkaEventPublisher    â”‚  â”‚ â€¢ KafkaConfig               â”‚â”‚
â”‚  â”‚   (with retry logic)     â”‚  â”‚ â€¢ AwsConfig                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ WebhooksProperties        â”‚â”‚
â”‚                 â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                           â”‚
    â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DynamoDB    â”‚          â”‚   Apache Kafka  â”‚
â”‚ (Schema      â”‚          â”‚   (AWS MSK)     â”‚
â”‚  Registry)   â”‚          â”‚  Topic Pattern: â”‚
â”‚              â”‚          â”‚  wh.ingress.    â”‚
â”‚ Table:       â”‚          â”‚  {domain}.      â”‚
â”‚ event_schema â”‚          â”‚  {eventName}    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Core Components

### 1. Controller Layer (`controller` package)

#### **EventController.java**
Location: `api/src/main/java/com/beewaxus/webhooksvcs/pubsrc/controller/EventController.java:41`

**Endpoints:**
- **Primary**: `POST /events/{eventName}` - Publishes events with domain/version headers
- **Secondary**: `POST /events/schema_id/{schemaId}` - Publishes events by schema ID

**Flow:**
1. Receives payload in JSON/XML/Avro format
2. Fetches schema from DynamoDB (via SchemaService)
3. Converts payload to JSON
4. Coerces types to match Avro schema (handles XML string conversion)
5. Validates against Avro schema
6. Serializes to Avro binary format
7. Publishes to Kafka with retry logic
8. Returns 202 Accepted with event ID

**Key Methods:**
- `publishEvent()` - Main event publishing endpoint
- `publishEventBySchemaId()` - Schema ID-based publishing
- `convertToJson()` - Format conversion (XML/JSON/Avro â†’ JSON)
- `coerceJsonToAvroTypes()` - Type coercion for Avro compatibility

---

#### **SchemaController.java**
Location: `api/src/main/java/com/beewaxus/webhooksvcs/pubsrc/controller/SchemaController.java:22`

**Endpoints:**
- `GET /schemas/{domain}/{event}/{version}` - Retrieves specific schema
- `GET /events/all` - Lists all active schemas
- `GET /events/schema_id/{schemaId}` - Retrieves schema by ID

---

#### **CacheController.java**
Location: `api/src/main/java/com/beewaxus/webhooksvcs/pubsrc/controller/CacheController.java:12`

**Endpoints:**
- `POST /cache/evict/all` - Evicts and reloads schema caches

---

### 2. Schema Service Layer (`schema` package)

#### **CachingSchemaService.java**
Location: `api/src/main/java/com/beewaxus/webhooksvcs/pubsrc/schema/CachingSchemaService.java:16`

**Pattern:** Decorator pattern with Caffeine in-memory cache

**Features:**
- Two separate caches:
  - `schemaCache`: Maps SchemaReference â†’ SchemaDefinition
  - `schemaDetailCache`: Maps schemaId â†’ SchemaDetailResponse
- Configurable TTL (default 24 hours)
- Configurable max entries (default 1000)
- Can be toggled on/off via `webhooks.cache.enabled`

**Key Methods:**
- `fetchSchema(SchemaReference)` - Cached schema lookup
- `fetchSchemaBySchemaId(String)` - Cached schema ID lookup
- `evictAndReload()` - Cache management

---

#### **DynamoSchemaService.java**
Location: `api/src/main/java/com/beewaxus/webhooksvcs/pubsrc/schema/DynamoSchemaService.java:24`

**Responsibilities:**
- Direct DynamoDB queries to `event_schema` table
- Composite key structure:
  - **PK**: `SCHEMA#{domain}#{eventName}`
  - **SK**: `v{version}`

**Query Methods:**
- `fetchSchema()` - Query by composite key (PK + SK)
- `fetchAllSchemas()` - Full table scan (cached)
- `fetchSchemaBySchemaId()` - Scan with filter expression

---

### 3. Publisher Layer (`publisher` package)

#### **KafkaEventPublisher.java**
Location: `api/src/main/java/com/beewaxus/webhooksvcs/pubsrc/publisher/KafkaEventPublisher.java:21`

**Features:**
- Two KafkaTemplates:
  - `kafkaTemplate`: For String payloads
  - `avroKafkaTemplate`: For byte[] (Avro binary)

**Retry Logic:**
- Max retries: 2 (configurable)
- Exponential backoff starting at 1 second
- Timeout: 30 seconds (configurable)
- Retries on: TimeoutException, RetriableException

**Topic Naming:**
- Pattern: `{prefix}.{domain}.{eventName}`
- Example: `wh.ingress.payments.transactionCreated`

**Key Methods:**
- `publish(EventEnvelope)` - Publish string payload
- `publishAvro(EventEnvelope, String, byte[])` - Publish Avro binary

---

### 4. Validation Layer (`validation` package)

#### **AvroSchemaValidator.java**
Location: `api/src/main/java/com/beewaxus/webhooksvcs/pubsrc/validation/AvroSchemaValidator.java:16`

**Responsibilities:**
- Converts JSON payloads to Avro GenericRecord
- Validates against Avro schema using `GenericData.get().validate()`
- Runs on boundedElastic scheduler (non-blocking I/O)

**Key Methods:**
- `validate(JsonNode, SchemaDefinition)` - Returns validated GenericRecord

---

### 5. Conversion Layer (`converter` package)

#### **FormatConverter.java**
- Converts XML â†’ JSON using Jackson XML mapper
- Handles XML to JSON transformation for validation

#### **AvroSerializer.java**
- Serializes GenericRecord â†’ Avro binary bytes
- Uses Avro's BinaryEncoder for efficient serialization

---

## ğŸ”„ Data Flow

### Event Publishing Flow (POST /events/{eventName})

```
1. Request arrives â†’ EventController.publishEvent()
   Headers: X-Producer-Domain, X-Event-Version, Content-Type
   Body: JSON/XML/Avro payload

2. Build SchemaReference(domain, eventName, version)

3. Fetch schema â†’ CachingSchemaService
   â”œâ”€ Cache hit? â†’ Return cached
   â””â”€ Cache miss? â†’ DynamoSchemaService.fetchSchema()
       â””â”€ Query DynamoDB: PK=SCHEMA#{domain}#{eventName}, SK=v{version}

4. Convert payload to JSON (if XML/Avro)
   â””â”€ FormatConverter.xmlToJson()

5. Type coercion â†’ coerceJsonToAvroTypes()
   â””â”€ Matches JSON types to Avro schema types (handles XML string types)

6. Validate â†’ AvroSchemaValidator.validate()
   â””â”€ Converts JSON â†’ GenericRecord & validates

7. Serialize â†’ AvroSerializer.serializeToAvro()
   â””â”€ GenericRecord â†’ byte[]

8. Publish â†’ KafkaEventPublisher.publishAvro()
   â”œâ”€ Topic: wh.ingress.{domain}.{eventName}
   â”œâ”€ Key: eventId
   â”œâ”€ Value: Avro binary bytes
   â””â”€ Retry with exponential backoff (max 2 retries, 30s timeout)

9. Return 202 Accepted { "eventId": "..." }
```

---

### Schema Lookup Flow (GET /schemas/{domain}/{event}/{version})

```
1. Request arrives â†’ SchemaController.getSchema()

2. Build SchemaReference(domain, event, version)

3. CachingSchemaService.fetchSchema()
   â”œâ”€ Check schemaCache
   â”œâ”€ Cache hit? â†’ Return cached SchemaDefinition
   â””â”€ Cache miss? â†’
       â”œâ”€ Query DynamoDB via DynamoSchemaService
       â”œâ”€ Filter for ACTIVE status
       â”œâ”€ Cache result
       â””â”€ Return SchemaDefinition

4. Return 200 OK with schema metadata
```

---

## ğŸ—„ï¸ DynamoDB Schema

**Table**: `event_schema`

| Attribute | Type | Description |
|-----------|------|-------------|
| **PK** | String | Partition Key: `SCHEMA#{domain}#{eventName}` |
| **SK** | String | Sort Key: `v{version}` |
| EVENT_SCHEMA_ID | String | Unique schema ID (e.g., SCHEMA_0001) |
| PRODUCER_DOMAIN | String | Domain name (e.g., payments) |
| EVENT_NAME | String | Event name (e.g., transactionCreated) |
| VERSION | String | Version (e.g., 1.0) |
| EVENT_SCHEMA_DEFINITION | String | Avro schema definition (primary) |
| EVENT_SCHEMA_DEFINITION_AVRO | String | Avro schema (fallback) |
| EVENT_SAMPLE | String | Sample event payload |
| EVENT_SCHEMA_STATUS | String | ACTIVE/INACTIVE |
| HAS_SENSITIVE_DATA | String | YES/NO |
| TOPIC_NAME | String | Kafka topic name |
| TOPIC_STATUS | String | ACTIVE/INACTIVE |
| INSERT_TS | String | ISO-8601 timestamp |
| INSERT_USER | String | User who created |
| UPDATE_TS | String | ISO-8601 timestamp |
| UPDATE_USER | String | User who updated |

**Access Patterns:**
1. **Get schema by reference**: Query on PK + SK
2. **Get schema by ID**: Scan with filter `EVENT_SCHEMA_ID = :schemaId`
3. **List all schemas**: Full table scan (cached)

---

## âš™ï¸ Configuration

**application.yaml**

```yaml
spring:
  application:
    name: event-validation-api
  main:
    web-application-type: reactive

webhooks:
  dynamodb:
    table-name: event_schema
  kafka:
    bootstrap-servers: localhost:9092
    ingress-topic-prefix: wh.ingress
    publish-timeout: PT30S  # 30 seconds
    max-retries: 2
    retry-backoff-initial-delay: PT1S
  cache:
    enabled: true
    schema-ttl: PT24H  # 24 hours
    schema-detail-ttl: PT24H
    maximum-entries: 1000

aws:
  region: us-east-1
  dynamodb:
    endpoint: http://localhost:8000  # For local testing

logging:
  level:
    root: INFO
    com.beewaxus.webhooksvcs.pubsrc: DEBUG
```

---

## ğŸ¯ Key Features

### 1. Multi-Format Support
- **Input formats**: JSON, XML, Avro
- **Output format**: Avro binary (for Kafka)
- Automatic conversion and type coercion

### 2. Schema Caching
- **Cache provider**: Caffeine (in-memory)
- **Cache levels**: Schema definitions + detailed responses
- **TTL**: Configurable (default 24 hours)
- **Eviction**: Manual via `/cache/evict/all` endpoint

### 3. Reactive Architecture
- **Framework**: Spring WebFlux (Project Reactor)
- **Benefits**: Non-blocking I/O, high throughput
- **Schedulers**: boundedElastic for blocking operations

### 4. Resilience Patterns
- **Kafka retry**: Exponential backoff with max retries
- **Timeout handling**: Configurable publish timeout
- **Error mapping**: Comprehensive exception handling

### 5. Type Coercion
- Handles XML (all strings) â†’ Avro type conversion
- Supports nested records, arrays, maps, unions
- Preserves type safety for Kafka consumers

### 6. Two Publishing Modes
- **Mode 1**: By domain/event/version (via headers)
- **Mode 2**: By schema ID (single identifier)

---

## ğŸ“Š Technology Stack

| Layer | Technology |
|-------|-----------|
| **Framework** | Spring Boot 3.3.1, WebFlux |
| **Language** | Java 21 |
| **Messaging** | Spring Kafka â†’ AWS MSK |
| **Database** | AWS DynamoDB (SDK v2.20.26) |
| **Caching** | Caffeine 3.1.8 |
| **Serialization** | Apache Avro 1.11.3, Jackson |
| **Validation** | JSON Schema Validator 1.5.2 |
| **Build** | Maven (multi-module) |

---

## ğŸš€ API Endpoints Summary

| Method | Endpoint | Purpose | Request | Response |
|--------|----------|---------|---------|----------|
| POST | `/events/{eventName}` | Publish event | Headers: X-Producer-Domain, X-Event-Version<br>Body: JSON/XML/Avro | 202 Accepted |
| POST | `/events/schema_id/{schemaId}` | Publish by schema ID | Body: JSON/XML/Avro | 202 Accepted |
| GET | `/schemas/{domain}/{event}/{version}` | Get specific schema | - | 200 OK |
| GET | `/events/all` | List all active schemas | - | 200 OK |
| GET | `/events/schema_id/{schemaId}` | Get schema by ID | - | 200 OK |
| POST | `/cache/evict/all` | Evict cache | - | 200 OK |

---

## ğŸ” Error Handling

### HTTP Status Codes

| Code | Scenario |
|------|----------|
| **202 Accepted** | Event validated and published successfully |
| **400 Bad Request** | Validation failed, missing required fields, inactive schema/topic |
| **404 Not Found** | Schema not found for given reference or ID |
| **500 Internal Server Error** | DynamoDB table not found, unexpected errors |
| **503 Service Unavailable** | Kafka unavailable, DynamoDB unavailable, timeout |

### Exception Mapping

- `SchemaValidationException` â†’ 400 Bad Request
- `DynamoDbException` â†’ 500/503 (based on error type)
- `KafkaPublishException` â†’ 503 Service Unavailable
- `TimeoutException` â†’ 503 Service Unavailable

---

## ğŸ“ˆ Scalability Considerations

### Horizontal Scaling
- Stateless design enables horizontal scaling
- Cache is local to each instance (trade-off)
- Reactive architecture supports high concurrency per instance

### Performance Optimization
- **Caching**: Reduces DynamoDB queries by ~95%
- **Reactive streams**: Non-blocking I/O maximizes throughput
- **Batch operations**: DynamoDB scan cached at service startup

### Bottlenecks
- **DynamoDB**: Query latency, provisioned capacity
- **Kafka**: Broker availability, topic partition count
- **Schema validation**: CPU-bound Avro validation

---

## ğŸ”„ Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Application Load Balancer              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚
    â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Instanceâ”‚         â”‚ Instanceâ”‚
â”‚    1    â”‚         â”‚    2    â”‚
â”‚ (cache) â”‚         â”‚ (cache) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚                   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         â”‚
    â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DynamoDB â”‚            â”‚   MSK    â”‚
â”‚  Table   â”‚            â”‚ Cluster  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Recommendations:**
- Deploy in ECS/EKS for container orchestration
- Use Application Load Balancer for traffic distribution
- Configure auto-scaling based on CPU/memory metrics
- Monitor Kafka lag and DynamoDB throttling

---

## ğŸ§ª Testing Strategy

### Unit Tests
- Mock SchemaService and EventPublisher
- Test type coercion logic
- Test error handling paths

### Integration Tests
- Use Testcontainers for DynamoDB Local + Kafka
- Test end-to-end event publishing flow
- Verify Avro serialization correctness

### Load Tests
- Measure throughput (events/sec)
- Test cache effectiveness
- Identify Kafka/DynamoDB bottlenecks

---

## ğŸ“š Additional Resources

- **OpenAPI Spec**: `oas-spec/src/main/resources/openapi/event-validation-api.yml`
- **Main README**: `README.md`
- **API README**: `api/README.md`
- **Postman Collection**: `postman/` directory

---

## ğŸ Conclusion

This architecture implements a robust, scalable event gateway with:
- âœ… Multi-format payload support
- âœ… Schema validation and versioning
- âœ… High-performance caching
- âœ… Resilient Kafka publishing
- âœ… Reactive, non-blocking design

The system is designed for **high-throughput webhook ingestion** with strong validation guarantees and operational observability.
